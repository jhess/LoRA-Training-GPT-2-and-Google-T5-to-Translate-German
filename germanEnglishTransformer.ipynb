{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_metric, load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from transformers import pipeline, TrainingArguments, Trainer, DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import evaluate\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from src.envhelpers import set_env, det_gpu_status\n",
    "from src.helpers import print_all_model_parameters, print_module_blocks, print_named_parameters, print_model_modules, print_model_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n",
      "Memory allocated: 0\n",
      "Max memory allocated: 0\n"
     ]
    }
   ],
   "source": [
    "set_env()\n",
    "train_on_gpu = det_gpu_status()\n",
    "import os\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = './certs/concatenated SSL bundle cert.cert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"de\"\n",
    "langkeys = {\"en\" : \"English\", \"de\" : \"German\"}\n",
    "\n",
    "# Load the opus dataset for German-English translations - there is no \"en-de\" dataset, so need to use \"de-en\"\n",
    "germanEnglishDataset = load_dataset(\"Helsinki-NLP/opus-100\", f\"{target_lang}-{source_lang}\", split=\"train[:150]\")\n",
    "#method 1a\n",
    "#germanEnglishDataset = load_dataset(\"Helsinki-NLP/opus-100\", f\"{source_lang}-{target_lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example translation pair:\n",
      "German: Sie w√ºrden herausfinden wollen: ,Bin ich allein?\"\n",
      "English: You're attempting to find, \"Am I alone?\"\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "# This is a Dataset object, so we can call test_train_split\n",
    "# germanEnglishDataset[\"train\"] is a dictionary that has the \"Translation\" column, so can't use test_train_split on that object\n",
    "dataset = germanEnglishDataset.train_test_split(test_size=0.2)\n",
    "#method 1a\n",
    "# this somehow is a Dataset object though? If use method 1a above to load dataset\n",
    "# dataset = germanEnglishDataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# method 2\n",
    "# Access the specific splits for training, validation, or testing dictionaries\n",
    "# train_dataset = dataset['train']\n",
    "# validation_dataset = dataset['validation']\n",
    "# test_dataset = dataset['test']\n",
    "\n",
    "# Get a key, value pair from the first element of the dataset\n",
    "example_translation_pair = dataset[\"train\"]['translation'][0] # method 1\n",
    "#example_translation_pair = dataset['translation'][0] # method 1a\n",
    "#example_translation_pair = train_dataset[0] #method 2\n",
    "print(\"Example translation pair:\")\n",
    "print(\"German:\", example_translation_pair['de'])\n",
    "print(\"English:\", example_translation_pair['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justhess\\OneDrive - Agilent Technologies\\Documents\\Python\\ML stuff\\Udacity projects\\proj3\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "gpt2 = \"gpt2\"\n",
    "googleT5 = \"google-t5/t5-small\"\n",
    "\n",
    "# 4 bit quantization during storage, 32 bit during fine tuning attention weights\n",
    "qLoRA_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    load_in_8bit=False,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# GPT-2\n",
    "gpt2Tokenizer = AutoTokenizer.from_pretrained(gpt2)\n",
    "gpt2Model = AutoModelForCausalLM.from_pretrained(gpt2, quantization_config=qLoRA_config_4bit)\n",
    "\n",
    "# # Google T5\n",
    "t5Tokenizer = AutoTokenizer.from_pretrained(googleT5)\n",
    "t5Model = AutoModelForSeq2SeqLM.from_pretrained(googleT5, quantization_config=qLoRA_config_4bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '1': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '2': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '3': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '4': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '5': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '6': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '7': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '8': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '9': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '10': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "), '11': GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "#print_model_modules(gpt2Model)\n",
    "# Output -> Model name: gpt2\n",
    "# Module Name: transformer, Module Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
    "# Module Name: lm_head, Module Type: <class 'torch.nn.modules.linear.Linear'>\n",
    "# Modules found from _modules, make sure to call model.module\n",
    "\n",
    "#print_module_blocks(gpt2Model, \"transformer\")\n",
    "# Output -> ['wte', 'wpe', 'drop', 'h', 'ln_f']\n",
    "print_module_blocks(gpt2Model.transformer, \"h\")\n",
    "# Output -> {'0': GPT2Block((ln_1):...(attn)..., '1': GPT2Block((ln_1):...(attn)...,...}\n",
    "\n",
    "# base_model and transformer are the same property for this model\n",
    "#print(gpt2Model.base_model.h[-1].attn)\n",
    "#print(gpt2Model.transformer.h[-1].attn)\n",
    "\n",
    "# Get the last layer/block Self Attention heads from each model - example for the names of the matrix parameters we want to tune\n",
    "# print(gpt2Model.base_model.h[-1].attn)\n",
    "# print(t5Model.base_model.decoder.block[-1].layer[0].SelfAttention)\n",
    "\n",
    "# # Print the attention blocks and their weight parameters in the Encoder\n",
    "# # The T5 Model's Decoder has 6 T5Stack object, each of which has a ModuleList that has 6 T5Block objects, which represent a layer\n",
    "\n",
    "# print_named_parameters(gpt2Model.base_model.h[-1].attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"translate {langkeys[f'{source_lang}']} to {langkeys[f'{target_lang}']}: \"\n",
    "#prefix = f\"translate {langkeys[f'{target_lang}']} to {langkeys[f'{source_lang}']}: \"\n",
    "\n",
    "def preprocess(examples) -> tuple[list[str], list[str]]:\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    # Do tokenization separately as need different tokenizer for T5, GPT2\n",
    "    #tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    #model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return inputs, targets\n",
    "\n",
    "def tokenizeT5(tokenizer, input_tuple: tuple[list[str], list[str]]):\n",
    "    inputs, targets = input_tuple\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=50, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "# Setting the padding=max_length seems required or an error will get thrown for GPT2, but takes more memory\n",
    "def tokenizeGpt(tokenizer, input_tuple: tuple[list[str], list[str]]):\n",
    "    inputs, targets = input_tuple\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=15, padding=\"max_length\", truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [00:00<00:00, 5002.75 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 1930.43 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [00:00<00:00, 5998.15 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 1873.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Repeat for both tokenizers\n",
    "\n",
    "gpt2Tokenizer.pad_token = gpt2Tokenizer.eos_token\n",
    "\n",
    "# Or add a new `[PAD]` token as the padding token - this doesn't work for GPT tokenizer\n",
    "#gpt2Tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# method 1 - this will have \"test\" and \"train\" columns from test_train_split (on original train column) to separately input to Trainer function\n",
    "tokenized_examples_t5 = dataset.map(lambda example: tokenizeT5(t5Tokenizer, preprocess(example)), batched=True)\n",
    "\n",
    "tokenized_examples_gpt = dataset.map(lambda example: tokenizeGpt(gpt2Tokenizer, preprocess(example)), batched=True)\n",
    "\n",
    "# method 2\n",
    "# tokenized_train = train_dataset.map(lambda example: tokenize(gpt2Tokenizer, preprocess(example)), batched=True)\n",
    "# tokenized_test = test_dataset.map(lambda example: tokenize(gpt2Tokenizer, preprocess(example)), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Performance on Model using a Pipeline and a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate English to German: Had he arrived already, I would have heard him enter the store, but apparently he never showed up.\n",
      "Translation: H√§tte er bereits angekommen, h√§tte ich ihn in den Laden h√∂ren k√∂nnen, aber er hat sich offenbar nie gezeigt.\n",
      "Reference: W√§re er schon da gewesen, h√§tte ich geh√∂rt, wie er den Laden betrat, aber anscheinend ist er nie aufgetaucht.\n",
      "SacreBLEU score: 10.110173740732797\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "sample_text1 = f\"{prefix}Had he arrived already, I would have heard him enter the store, but apparently he never showed up.\"\n",
    "sample_text2 = f\"{prefix}I would have liked to have saved them both\"\n",
    "\n",
    "# Try to reverse translate target language to source to evaluate performance\n",
    "translator = pipeline(f\"translation_{target_lang}_to_{source_lang}\", model=googleT5, max_length=400, device=0) # GPU\n",
    "\n",
    "reference1 = \"W√§re er schon da gewesen, h√§tte ich geh√∂rt, wie er den Laden betrat, aber anscheinend ist er nie aufgetaucht.\"\n",
    "reference2 = \"Ich h√§tte gern beide gerettet\"\n",
    "\n",
    "translated_sentence1 = translator(sample_text1)\n",
    "translation = translated_sentence1[0][\"translation_text\"]\n",
    "\n",
    "# Compute the SacreBLEU score for the translated output\n",
    "sacrebleu_score = sacrebleu_metric.compute(predictions=[translation],\n",
    "                                            references=[[reference1]])\n",
    "\n",
    "# Display the SacreBLEU score for the translation\n",
    "print(sample_text1)\n",
    "print(f\"Translation: {translation}\")\n",
    "print(f\"Reference: {reference1}\")\n",
    "print(\"SacreBLEU score:\", sacrebleu_score[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds, metric, tokenizer):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justhess\\OneDrive - Agilent Technologies\\Documents\\Python\\ML stuff\\Udacity projects\\proj3\\venv\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/120 [00:00<?, ?it/s]c:\\Users\\justhess\\OneDrive - Agilent Technologies\\Documents\\Python\\ML stuff\\Udacity projects\\proj3\\venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                                                 \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [00:12<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 0.9439, 'eval_samples_per_second': 31.783, 'eval_steps_per_second': 31.783, 'epoch': 1.0}\n",
      "{'train_runtime': 12.1007, 'train_samples_per_second': 9.917, 'train_steps_per_second': 9.917, 'train_loss': 9.059440104166667, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 DataCollator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=gpt2Tokenizer)\n",
    "\n",
    "gpt2config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\",\n",
    "                    \"c_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    #modules_to_save=[\"classifier\"],\n",
    ")\n",
    "lora_gpt2Model = get_peft_model(gpt2Model, gpt2config)\n",
    "\n",
    "# Define the path to the directory where the training artifacts are stored\n",
    "training_args_path_gpt = f\"hess_german_to_english_{gpt2}\"\n",
    "\n",
    "# GPT-2 Training\n",
    "\n",
    "#lora_gpt2Model.to(\"cuda\" if train_on_gpu else \"cpu\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= training_args_path_gpt,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5, #5e-3\n",
    "    per_device_train_batch_size=1, #128, or 4\n",
    "    per_device_eval_batch_size=1, # or 2\n",
    "    weight_decay=0.01,\n",
    "    # save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    # fp16=True,\n",
    "    # push_to_hub=False\n",
    ")\n",
    "\n",
    "# Retrieve the path to the trained model from the TrainingArguments\n",
    "gpt2_model_path = training_args.output_dir\n",
    "\n",
    "partial_compute_metrics = partial(compute_metrics, metric=sacrebleu_metric, tokenizer=gpt2Tokenizer)\n",
    "\n",
    "# Load LoRA model\n",
    "trainer = Trainer(\n",
    "    model=lora_gpt2Model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_examples_gpt[\"train\"],\n",
    "    eval_dataset=tokenized_examples_gpt[\"test\"],\n",
    "    tokenizer=gpt2Tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=partial_compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the transformer on the tokenized train and test data\n",
    "try:\n",
    "\n",
    "   trainer.train()\n",
    "   #trainer.evaluate()\n",
    "   \n",
    "except RuntimeError as e:\n",
    "   if 'CUDA out of memory.' in str(e):\n",
    "        print(\"CUDA out of memory error occured\")\n",
    "        torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 33.16it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "   trainer.evaluate()\n",
    "   \n",
    "except RuntimeError as e:\n",
    "   if 'CUDA out of memory.' in str(e):\n",
    "        print(\"CUDA out of memory error occured\")\n",
    "        torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq DataCollator for TrainingArguments object input\n",
    "data_collator_seq_2_seq = DataCollatorForSeq2Seq(tokenizer=t5Tokenizer, model=googleT5)\n",
    "\n",
    "t5config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\",\n",
    "                    \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"lora_only\",\n",
    "    modules_to_save=[\"decode_head\"],\n",
    ")\n",
    "lora_t5Model = get_peft_model(t5Model, t5config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justhess\\OneDrive - Agilent Technologies\\Documents\\Python\\ML stuff\\Udacity projects\\proj3\\venv\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "  5%|‚ñå         | 121/2400 [00:14<21:43,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.5443, 'eval_samples_per_second': 19.426, 'eval_steps_per_second': 19.426, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|‚ñà         | 241/2400 [00:28<11:27,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3941, 'eval_samples_per_second': 21.52, 'eval_steps_per_second': 21.52, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 15%|‚ñà‚ñå        | 361/2400 [00:41<10:40,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3786, 'eval_samples_per_second': 21.761, 'eval_steps_per_second': 21.761, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|‚ñà‚ñà        | 481/2400 [00:55<11:38,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3908, 'eval_samples_per_second': 21.57, 'eval_steps_per_second': 21.57, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 500/2400 [00:56<03:15,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3234, 'grad_norm': 0.3837828040122986, 'learning_rate': 1.5833333333333333e-05, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 25%|‚ñà‚ñà‚ñå       | 601/2400 [01:09<13:22,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4, 'eval_samples_per_second': 21.429, 'eval_steps_per_second': 21.429, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 30%|‚ñà‚ñà‚ñà       | 722/2400 [01:22<08:42,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3998, 'eval_samples_per_second': 21.431, 'eval_steps_per_second': 21.431, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 842/2400 [01:36<07:03,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3781, 'eval_samples_per_second': 21.77, 'eval_steps_per_second': 21.77, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 960/2400 [01:49<02:23, 10.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4263, 'eval_samples_per_second': 21.034, 'eval_steps_per_second': 21.034, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1000/2400 [01:53<02:19, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3371, 'grad_norm': 1.1871904134750366, 'learning_rate': 1.1666666666666668e-05, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1081/2400 [02:03<11:40,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4195, 'eval_samples_per_second': 21.134, 'eval_steps_per_second': 21.134, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1202/2400 [02:16<05:27,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3981, 'eval_samples_per_second': 21.458, 'eval_steps_per_second': 21.458, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1320/2400 [02:30<01:47, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3737, 'eval_samples_per_second': 21.838, 'eval_steps_per_second': 21.838, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1442/2400 [02:43<04:58,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3867, 'eval_samples_per_second': 21.634, 'eval_steps_per_second': 21.634, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1500/2400 [02:49<01:29, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2501, 'grad_norm': 0.2790587544441223, 'learning_rate': 7.500000000000001e-06, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1561/2400 [02:57<05:11,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3772, 'eval_samples_per_second': 21.783, 'eval_steps_per_second': 21.783, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1680/2400 [03:10<01:11, 10.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4027, 'eval_samples_per_second': 21.387, 'eval_steps_per_second': 21.387, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1801/2400 [03:24<03:06,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3696, 'eval_samples_per_second': 21.905, 'eval_steps_per_second': 21.905, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1921/2400 [03:37<02:27,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3824, 'eval_samples_per_second': 21.701, 'eval_steps_per_second': 21.701, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2000/2400 [03:45<00:40,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2847, 'grad_norm': 0.8167466521263123, 'learning_rate': 3.3333333333333333e-06, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2041/2400 [03:51<02:53,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3841, 'eval_samples_per_second': 21.675, 'eval_steps_per_second': 21.675, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2161/2400 [04:04<02:14,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.5231, 'eval_samples_per_second': 19.696, 'eval_steps_per_second': 19.696, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 2280/2400 [04:18<00:11, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3885, 'eval_samples_per_second': 21.606, 'eval_steps_per_second': 21.606, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2400/2400 [04:32<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4069, 'eval_samples_per_second': 21.324, 'eval_steps_per_second': 21.324, 'epoch': 20.0}\n",
      "{'train_runtime': 272.0043, 'train_samples_per_second': 8.823, 'train_steps_per_second': 8.823, 'train_loss': 2.285186462402344, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:01<00:00, 21.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the directory where the training artifacts are stored\n",
    "training_args_path_t5 = f\"hess_german_to_english_{googleT5}\"\n",
    "\n",
    "# Google T-5 Training\n",
    "#lora_t5Model.to(\"cuda\" if train_on_gpu else \"cpu\")\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir= training_args_path_t5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    #save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    #predict_with_generate=True,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=False\n",
    ")\n",
    "\n",
    "# Retrieve the path to the trained model from the TrainingArguments\n",
    "t5_model_path = training_args.output_dir\n",
    "\n",
    "partial_compute_metrics = partial(compute_metrics, metric=sacrebleu_metric, tokenizer=t5Tokenizer)\n",
    "\n",
    "# Load LoRA model\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=lora_t5Model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_examples_t5[\"train\"],\n",
    "    eval_dataset=tokenized_examples_t5[\"test\"],\n",
    "    tokenizer=t5Tokenizer,\n",
    "    data_collator=data_collator_seq_2_seq,\n",
    "    compute_metrics=partial_compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the transformer on the tokenized train and test data\n",
    "try:\n",
    "\n",
    "   trainer.train()\n",
    "   trainer.evaluate()\n",
    "   \n",
    "except RuntimeError as e:\n",
    "   if 'CUDA out of memory.' in str(e):\n",
    "        print(\"CUDA out of memory error occured\")\n",
    "        torch.cuda.empty_cache()\n",
    "         \n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hess_german_to_english_google-t5/t5-small\n"
     ]
    }
   ],
   "source": [
    "lora_gpt2Model.save_pretrained(gpt2_model_path, from_pt=True) \n",
    "lora_t5Model.save_pretrained(t5_model_path, from_pt=True)\n",
    "print(t5_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text: str, model, tokenizer, use_peft=False) -> str:\n",
    "    \"\"\"\n",
    "    function to translate a text using a model and tokenizer\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # encode the text using the tokenizer\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "        # input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        attention_mask=inputs.ne(tokenizer.pad_token_id).float()\n",
    "        # generate the translation using the model\n",
    "        if use_peft:\n",
    "            outputs = model.generate(input_ids=inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95, attention_mask=attention_mask)\n",
    "        else:\n",
    "            outputs = model.generate(**inputs)\n",
    "\n",
    "        # decode the generated output and return the translated text\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate English to German: Had he arrived already, I would have heard him enter the store, but apparently he never showed up.\n",
      "\n",
      "Original Translation:\n",
      "H√§tte er bereits angekommen, h√§tte ich ihn in den Laden h√∂ren k√∂nnen, aber er hat sich offenbar nie gezeigt.\n",
      "\n",
      "Post-training Translation:\n",
      "Wenn er bereits gekommen w√§re, h√§tte ich ihn in den Laden geredet, aber es scheint, dass er nie aufgekommen ist.\n",
      "\n",
      "Reference: W√§re er schon da gewesen, h√§tte ich geh√∂rt, wie er den Laden betrat, aber anscheinend ist er nie aufgetaucht.\n",
      "SacreBLEU score: 10.624917754018972\n"
     ]
    }
   ],
   "source": [
    "# Try to reverse translate target language to source to evaluate performance\n",
    "#translator = pipeline(f\"translation_{target_lang}_to_{source_lang}\", model=googleT5, device=0) # GPU\n",
    "# Or...manually replocate the results of the pipeline:\n",
    "\n",
    "# Load the PEFT model from the saved directory from the Training args output_dir where the model was saved\n",
    "#gpt2_peft_model = TFPegasusForConditionalGeneration.from_pretrained(gpt2_model_path)\n",
    "t5_peft_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
    "\n",
    "# Evaluate T5 to translate original text post-LoRA and qLoRA training\n",
    "# NOTE: GPT-2 does not perform well on translation task, so use T5 model\n",
    "translation1 = translate(sample_text1, t5_peft_model, t5Tokenizer, use_peft=True)\n",
    "#translation1 = translate(sample_text1, lora_t5Model, t5Tokenizer, use_peft=True)\n",
    "\n",
    "# Compute the SacreBLEU score for the translated output\n",
    "sacrebleu_score = sacrebleu_metric.compute(predictions=[translation1],\n",
    "                                            references=[[reference1]])\n",
    "\n",
    "# Display the SacreBLEU score for the translation\n",
    "print(sample_text1 + \"\\n\")\n",
    "print(f\"Original Translation:\" + \"\\n\" + f\"{translation}\" + \"\\n\")\n",
    "print(f\"Post-training Translation:\" + \"\\n\" + f\"{translation1}\" + \"\\n\")\n",
    "print(f\"Reference: {reference1}\")\n",
    "print(\"SacreBLEU score:\", sacrebleu_score[\"score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
